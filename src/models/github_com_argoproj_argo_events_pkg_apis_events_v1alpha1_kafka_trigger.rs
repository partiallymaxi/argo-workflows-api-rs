/*
 * Argo Workflows API
 *
 * Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. For more information, please see https://argo-workflows.readthedocs.io/en/latest/
 *
 * The version of the OpenAPI document: VERSION
 * 
 * Generated by: https://openapi-generator.tech
 */

use crate::models;
use serde::{Deserialize, Serialize};

/// GithubComArgoprojArgoEventsPkgApisEventsV1alpha1KafkaTrigger : KafkaTrigger refers to the specification of the Kafka trigger.
#[derive(Clone, Default, Debug, PartialEq, Serialize, Deserialize)]
pub struct GithubComArgoprojArgoEventsPkgApisEventsV1alpha1KafkaTrigger {
    #[serde(rename = "compress", skip_serializing_if = "Option::is_none")]
    pub compress: Option<bool>,
    #[serde(rename = "flushFrequency", skip_serializing_if = "Option::is_none")]
    pub flush_frequency: Option<i32>,
    #[serde(rename = "headers", skip_serializing_if = "Option::is_none")]
    pub headers: Option<std::collections::HashMap<String, String>>,
    /// Parameters is the list of parameters that is applied to resolved Kafka trigger object.
    #[serde(rename = "parameters", skip_serializing_if = "Option::is_none")]
    pub parameters: Option<Vec<models::GithubComArgoprojArgoEventsPkgApisEventsV1alpha1TriggerParameter>>,
    #[serde(rename = "partition", skip_serializing_if = "Option::is_none")]
    pub partition: Option<i32>,
    /// The partitioning key for the messages put on the Kafka topic. +optional.
    #[serde(rename = "partitioningKey", skip_serializing_if = "Option::is_none")]
    pub partitioning_key: Option<String>,
    /// Payload is the list of key-value extracted from an event payload to construct the request payload.
    #[serde(rename = "payload", skip_serializing_if = "Option::is_none")]
    pub payload: Option<Vec<models::GithubComArgoprojArgoEventsPkgApisEventsV1alpha1TriggerParameter>>,
    /// RequiredAcks used in producer to tell the broker how many replica acknowledgements Defaults to 1 (Only wait for the leader to ack). +optional.
    #[serde(rename = "requiredAcks", skip_serializing_if = "Option::is_none")]
    pub required_acks: Option<i32>,
    #[serde(rename = "sasl", skip_serializing_if = "Option::is_none")]
    pub sasl: Option<Box<models::GithubComArgoprojArgoEventsPkgApisEventsV1alpha1SaslConfig>>,
    #[serde(rename = "schemaRegistry", skip_serializing_if = "Option::is_none")]
    pub schema_registry: Option<Box<models::GithubComArgoprojArgoEventsPkgApisEventsV1alpha1SchemaRegistryConfig>>,
    #[serde(rename = "secureHeaders", skip_serializing_if = "Option::is_none")]
    pub secure_headers: Option<Vec<models::GithubComArgoprojArgoEventsPkgApisEventsV1alpha1SecureHeader>>,
    #[serde(rename = "tls", skip_serializing_if = "Option::is_none")]
    pub tls: Option<Box<models::GithubComArgoprojArgoEventsPkgApisEventsV1alpha1TlsConfig>>,
    #[serde(rename = "topic", skip_serializing_if = "Option::is_none")]
    pub topic: Option<String>,
    /// URL of the Kafka broker, multiple URLs separated by comma.
    #[serde(rename = "url", skip_serializing_if = "Option::is_none")]
    pub url: Option<String>,
    #[serde(rename = "version", skip_serializing_if = "Option::is_none")]
    pub version: Option<String>,
}

impl GithubComArgoprojArgoEventsPkgApisEventsV1alpha1KafkaTrigger {
    /// KafkaTrigger refers to the specification of the Kafka trigger.
    pub fn new() -> GithubComArgoprojArgoEventsPkgApisEventsV1alpha1KafkaTrigger {
        GithubComArgoprojArgoEventsPkgApisEventsV1alpha1KafkaTrigger {
            compress: None,
            flush_frequency: None,
            headers: None,
            parameters: None,
            partition: None,
            partitioning_key: None,
            payload: None,
            required_acks: None,
            sasl: None,
            schema_registry: None,
            secure_headers: None,
            tls: None,
            topic: None,
            url: None,
            version: None,
        }
    }
}

